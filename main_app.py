# =====================================================
# IMPORTS & CONFIG
# =====================================================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
import io
from datetime import datetime, timedelta
import calendar
from scipy import stats
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de p√°gina
st.set_page_config(
    page_title="Retail Sales Intelligent Dashboard", 
    layout="wide",
    page_icon="üìä"
)

# =====================================================
# CACHE FUNCTIONS
# =====================================================
@st.cache_data
def load_file(file):
    """Carga archivo CSV"""
    return pd.read_csv(file)

@st.cache_data
def load_url(url):
    """Carga datos desde URL"""
    try:
        if url.startswith(('http://', 'https://')):
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return pd.read_csv(io.StringIO(response.text))
    except Exception as e:
        st.error(f"Error cargando URL: {str(e)}")
        return None

# =====================================================
# ETL AVANZADO CON MANEJO DE VALORES FALTANTES
# =====================================================
def enhanced_clean_data(df, remove_duplicates=True, impute_method="Mediana", outlier_threshold=3.0):
    """
    Funci√≥n mejorada de limpieza de datos con imputaci√≥n avanzada
    y manejo inteligente de valores faltantes
    """
    transformations = []
    df_original = df.copy()
    
    # 1. AN√ÅLISIS INICIAL
    initial_rows = len(df)
    initial_cols = len(df.columns)
    initial_missing = df.isnull().sum().sum()
    
    transformations.append("üìä **AN√ÅLISIS INICIAL**")
    transformations.append(f"   ‚Ä¢ Registros: {initial_rows:,}")
    transformations.append(f"   ‚Ä¢ Columnas: {initial_cols}")
    transformations.append(f"   ‚Ä¢ Valores faltantes: {initial_missing:,}")
    
    # 2. CONVERSIONES DE TIPO DE DATOS
    transformations.append("\nüîÑ **CONVERSI√ìN DE TIPOS DE DATOS**")
    
    # Columnas num√©ricas
    numeric_cols = ['Price Per Unit', 'Quantity', 'Total Spent']
    for col in numeric_cols:
        if col in df.columns:
            non_numeric = df[col].apply(lambda x: isinstance(x, str) and not x.replace('.', '', 1).isdigit()).sum()
            if non_numeric > 0:
                transformations.append(f"   ‚Ä¢ {col}: {non_numeric} valores no num√©ricos convertidos")
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # Fechas
    date_cols = ['Transaction Date']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')
            invalid_dates = df[col].isnull().sum()
            if invalid_dates > 0:
                transformations.append(f"   ‚Ä¢ {col}: {invalid_dates} fechas inv√°lidas encontradas")
    
    # Variables categ√≥ricas
    categorical_cols = ['Category', 'Item', 'Payment Method', 'Location', 'Customer ID']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()
    
    # Variable booleana
    if 'Discount Applied' in df.columns:
        # Normalizar diferentes formatos booleanos
        discount_map = {
            'true': True, 'false': False,
            'yes': True, 'no': False,
            '1': True, '0': False,
            'verdadero': True, 'falso': False,
            'si': True, 's√≠': True
        }
        df['Discount Applied'] = (
            df['Discount Applied']
            .astype(str)
            .str.lower()
            .str.strip()
            .map(discount_map)
            .fillna(False)
        )
        transformations.append("   ‚Ä¢ 'Discount Applied' convertido a booleano")
    
    # 3. ELIMINACI√ìN DE DUPLICADOS
    if remove_duplicates:
        duplicates = df.duplicated().sum()
        df = df.drop_duplicates()
        transformations.append(f"\nüßπ **ELIMINACI√ìN DE DUPLICADOS**")
        transformations.append(f"   ‚Ä¢ {duplicates} registros duplicados eliminados")
    
    # 4. AN√ÅLISIS DETALLADO DE VALORES FALTANTES
    transformations.append("\nüîç **AN√ÅLISIS DE VALORES FALTANTES**")
    missing_analysis = df.isnull().sum()
    missing_percentage = (missing_analysis / len(df) * 100).round(2)
    
    for col in df.columns:
        if missing_analysis[col] > 0:
            transformations.append(f"   ‚Ä¢ {col}: {missing_analysis[col]:,} ({missing_percentage[col]}%)")
    
    # 5. ESTRATEGIAS DE IMPUTACI√ìN INTELIGENTE
    transformations.append("\nüîß **IMPUTACI√ìN INTELIGENTE**")
    
    # Estrategia 1: C√°lculos derivados
    calculated_count = 0
    if all(col in df.columns for col in ['Price Per Unit', 'Quantity', 'Total Spent']):
        # Caso 1: Calcular Total Spent a partir de Price y Quantity
        mask = df['Total Spent'].isnull() & df['Price Per Unit'].notnull() & df['Quantity'].notnull()
        if mask.sum() > 0:
            df.loc[mask, 'Total Spent'] = df.loc[mask, 'Price Per Unit'] * df.loc[mask, 'Quantity']
            calculated_count += mask.sum()
            transformations.append(f"   ‚Ä¢ Calculado 'Total Spent' para {mask.sum():,} registros")
        
        # Caso 2: Calcular Price Per Unit a partir de Total Spent y Quantity
        mask = df['Price Per Unit'].isnull() & df['Total Spent'].notnull() & df['Quantity'].notnull() & (df['Quantity'] != 0)
        if mask.sum() > 0:
            df.loc[mask, 'Price Per Unit'] = df.loc[mask, 'Total Spent'] / df.loc[mask, 'Quantity']
            calculated_count += mask.sum()
            transformations.append(f"   ‚Ä¢ Calculado 'Price Per Unit' para {mask.sum():,} registros")
        
        # Caso 3: Calcular Quantity a partir de Total Spent y Price
        mask = df['Quantity'].isnull() & df['Total Spent'].notnull() & df['Price Per Unit'].notnull() & (df['Price Per Unit'] != 0)
        if mask.sum() > 0:
            df.loc[mask, 'Quantity'] = df.loc[mask, 'Total Spent'] / df.loc[mask, 'Price Per Unit']
            calculated_count += mask.sum()
            transformations.append(f"   ‚Ä¢ Calculado 'Quantity' para {mask.sum():,} registros")
    
    # Estrategia 2: Imputaci√≥n por m√©todo seleccionado para valores num√©ricos
    imputed_numeric_count = 0
    for col in numeric_cols:
        if col in df.columns:
            missing_count = df[col].isnull().sum()
            if missing_count > 0:
                if impute_method == "Media":
                    impute_value = df[col].mean()
                    method_name = "media"
                elif impute_method == "Mediana":
                    impute_value = df[col].median()
                    method_name = "mediana"
                elif impute_method == "Cero":
                    impute_value = 0
                    method_name = "cero"
                elif impute_method == "Moda por Categor√≠a" and 'Category' in df.columns:
                    # Imputaci√≥n por categor√≠a
                    for category in df['Category'].unique():
                        if pd.notna(category):
                            cat_mean = df[df['Category'] == category][col].mean()
                            mask = (df['Category'] == category) & (df[col].isnull())
                            df.loc[mask, col] = cat_mean
                    impute_value = "por categor√≠a"
                    method_name = "moda por categor√≠a"
                else:
                    continue  # No imputar
                
                if impute_method != "Moda por Categor√≠a":
                    df[col] = df[col].fillna(impute_value)
                
                imputed_numeric_count += missing_count
                transformations.append(f"   ‚Ä¢ {col}: {missing_count:,} valores imputados con {method_name}")
    
    # Estrategia 3: Imputaci√≥n para variables categ√≥ricas
    imputed_categorical_count = 0
    for col in categorical_cols:
        if col in df.columns:
            missing_count = df[col].isnull().sum()
            if missing_count > 0:
                if col == 'Item' and 'Category' in df.columns:
                    # Para Item, usar la moda de la categor√≠a correspondiente
                    for category in df['Category'].unique():
                        if pd.notna(category):
                            category_items = df[df['Category'] == category]['Item'].dropna()
                            if not category_items.empty:
                                mode_value = category_items.mode().iloc[0] if not category_items.mode().empty else f"Unknown_{category}"
                                mask = (df['Category'] == category) & (df[col].isnull())
                                df.loc[mask, col] = mode_value
                    transformations.append(f"   ‚Ä¢ {col}: imputado usando moda por categor√≠a")
                else:
                    # Para otras variables categ√≥ricas, usar la moda global
                    mode_value = df[col].mode()
                    if not mode_value.empty:
                        df[col] = df[col].fillna(mode_value.iloc[0])
                        transformations.append(f"   ‚Ä¢ {col}: imputado con moda global")
                imputed_categorical_count += missing_count
    
    # Estrategia 4: Imputaci√≥n de fechas
    if 'Transaction Date' in df.columns:
        missing_dates = df['Transaction Date'].isnull().sum()
        if missing_dates > 0:
            # Imputar con fecha mediana del mismo per√≠odo
            median_date = df['Transaction Date'].median()
            df['Transaction Date'] = df['Transaction Date'].fillna(median_date)
            transformations.append(f"   ‚Ä¢ Fechas: {missing_dates:,} imputadas con fecha mediana")
    
    # 6. MANEJO DE OUTLIERS CON WINZORIZACI√ìN
    transformations.append("\nüìà **MANEJO DE OUTLIERS**")
    outlier_counts = {}
    
    for col in numeric_cols:
        if col in df.columns and df[col].notnull().any() and df[col].var() > 0:
            # Calcular l√≠mites usando percentiles (m√°s robusto que z-score)
            q1 = df[col].quantile(0.25)
            q3 = df[col].quantile(0.75)
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            # Identificar outliers
            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            
            if outliers > 0:
                outlier_counts[col] = outliers
                
                # Winsorizaci√≥n: limitar valores extremos
                df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
                df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    
    if outlier_counts:
        for col, count in outlier_counts.items():
            transformations.append(f"   ‚Ä¢ {col}: {count:,} outliers winsorizados")
    else:
        transformations.append("   ‚Ä¢ No se detectaron outliers significativos")
    
    # 7. VALIDACI√ìN DE INTEGRIDAD
    transformations.append("\n‚úÖ **VALIDACI√ìN DE INTEGRIDAD**")
    
    # Verificar Transaction ID √∫nico
    if 'Transaction ID' in df.columns:
        duplicates = df['Transaction ID'].duplicated().sum()
        if duplicates == 0:
            transformations.append("   ‚Ä¢ Transaction ID: √∫nico ‚úì")
        else:
            transformations.append(f"   ‚Ä¢ Transaction ID: {duplicates} duplicados encontrados ‚ö†Ô∏è")
    
    # Verificar valores negativos
    negative_issues = []
    for col in numeric_cols:
        if col in df.columns:
            negative_count = (df[col] < 0).sum()
            if negative_count > 0:
                # Corregir valores negativos
                df[col] = df[col].clip(lower=0)
                negative_issues.append(f"{col}: {negative_count:,}")
    
    if negative_issues:
        transformations.append(f"   ‚Ä¢ Valores negativos corregidos: {', '.join(negative_issues)}")
    else:
        transformations.append("   ‚Ä¢ Sin valores negativos ‚úì")
    
    # Verificar rangos razonables
    if 'Quantity' in df.columns:
        unreasonable_qty = (df['Quantity'] > 1000).sum()
        if unreasonable_qty > 0:
            transformations.append(f"   ‚Ä¢ Cantidades extremas: {unreasonable_qty:,} > 1000 ‚ö†Ô∏è")
    
    # 8. RESULTADOS FINALES
    final_missing = df.isnull().sum().sum()
    missing_reduction = initial_missing - final_missing
    
    transformations.append("\nüéØ **RESUMEN FINAL**")
    transformations.append(f"   ‚Ä¢ Registros finales: {len(df):,}")
    transformations.append(f"   ‚Ä¢ Valores faltantes eliminados: {missing_reduction:,}")
    transformations.append(f"   ‚Ä¢ Valores faltantes restantes: {final_missing:,}")
    
    if final_missing > 0:
        remaining_missing = df.isnull().sum()
        remaining_missing = remaining_missing[remaining_missing > 0]
        for col, count in remaining_missing.items():
            percentage = (count / len(df) * 100).round(2)
            transformations.append(f"   ‚Ä¢ {col}: {count:,} ({percentage}%) a√∫n faltantes")
    
    return df, df_original, transformations

# =====================================================
# AN√ÅLISIS AVANZADO
# =====================================================
def analyze_category_profitability(df):
    """Analiza rentabilidad por categor√≠a"""
    if "Category" not in df.columns or "Total Spent" not in df.columns:
        return None
    
    analysis = df.groupby("Category").agg({
        "Total Spent": ["sum", "mean", "count", "std"],
        "Quantity": ["sum", "mean"] if "Quantity" in df.columns else None,
        "Customer ID": "nunique" if "Customer ID" in df.columns else None
    }).round(2)
    
    # Aplanar columnas
    analysis.columns = ['_'.join(col).strip('_') for col in analysis.columns.values]
    
    # Renombrar columnas
    rename_dict = {
        "Total Spent_sum": "Ingreso_Total",
        "Total Spent_mean": "Ticket_Promedio",
        "Total Spent_count": "Transacciones",
        "Total Spent_std": "Desviacion_Ingreso",
        "Quantity_sum": "Cantidad_Total",
        "Quantity_mean": "Cantidad_Promedio",
        "Customer ID_nunique": "Clientes_Unicos"
    }
    
    for old, new in rename_dict.items():
        if old in analysis.columns:
            analysis = analysis.rename(columns={old: new})
    
    # Calcular m√©tricas adicionales
    analysis["%_Contribucion"] = (analysis["Ingreso_Total"] / analysis["Ingreso_Total"].sum() * 100).round(2)
    analysis["Margen_Estimado"] = (analysis["Ticket_Promedio"] * 0.3).round(2)  # Asumiendo 30% de margen
    
    # Ordenar por rentabilidad
    analysis = analysis.sort_values("Ingreso_Total", ascending=False)
    
    return analysis

def analyze_customer_segments(df):
    """Analiza segmentos de clientes avanzado"""
    segments = {}
    
    # RFM Analysis
    if all(col in df.columns for col in ["Customer ID", "Transaction Date", "Total Spent"]):
        rfm = df.groupby("Customer ID").agg({
            "Transaction Date": lambda x: (df["Transaction Date"].max() - x.max()).days,
            "Customer ID": "count",
            "Total Spent": "sum"
        }).rename(columns={
            "Transaction Date": "Recency",
            "Customer ID": "Frequency",
            "Total Spent": "Monetary"
        })
        
        # Crear segmentos RFM
        rfm["R_Score"] = pd.qcut(rfm["Recency"], 4, labels=[4, 3, 2, 1])
        rfm["F_Score"] = pd.qcut(rfm["Frequency"], 4, labels=[1, 2, 3, 4])
        rfm["M_Score"] = pd.qcut(rfm["Monetary"], 4, labels=[1, 2, 3, 4])
        
        rfm["RFM_Score"] = rfm["R_Score"].astype(str) + rfm["F_Score"].astype(str) + rfm["M_Score"].astype(str)
        
        # Segmentar clientes
        def segment_rfm(row):
            if row["RFM_Score"] in ["444", "443", "434", "344"]:
                return "Campeones"
            elif row["R_Score"] == 4 and row["F_Score"] >= 3:
                return "Leales"
            elif row["R_Score"] >= 3 and row["F_Score"] >= 3:
                return "Potenciales"
            elif row["R_Score"] >= 3:
                return "Prometedores"
            elif row["F_Score"] >= 3:
                return "Necesitan Atenci√≥n"
            else:
                return "En Riesgo"
        
        rfm["Segmento"] = rfm.apply(segment_rfm, axis=1)
        segments["rfm"] = rfm
    
    # An√°lisis por ubicaci√≥n
    if "Location" in df.columns:
        loc_analysis = df.groupby("Location").agg({
            "Total Spent": ["sum", "mean", "count"],
            "Customer ID": "nunique" if "Customer ID" in df.columns else None
        }).round(2)
        segments["ubicacion"] = loc_analysis
    
    # An√°lisis por m√©todo de pago
    if "Payment Method" in df.columns:
        pay_analysis = df.groupby("Payment Method").agg({
            "Total Spent": ["sum", "mean", "count", "std"],
            "Discount Applied": "mean" if "Discount Applied" in df.columns else None
        }).round(2)
        segments["metodo_pago"] = pay_analysis
    
    return segments

def analyze_temporal_patterns(df):
    """Analiza patrones temporales avanzados"""
    if "Transaction Date" not in df.columns:
        return None
    
    patterns = {}
    df_temp = df.copy()
    
    # Extraer componentes de tiempo
    df_temp["Year"] = df_temp["Transaction Date"].dt.year
    df_temp["Month"] = df_temp["Transaction Date"].dt.month
    df_temp["Month_Name"] = df_temp["Transaction Date"].dt.strftime('%B')
    df_temp["Day"] = df_temp["Transaction Date"].dt.day
    df_temp["Day_of_Week"] = df_temp["Transaction Date"].dt.dayofweek
    df_temp["Day_Name"] = df_temp["Transaction Date"].dt.strftime('%A')
    df_temp["Week"] = df_temp["Transaction Date"].dt.isocalendar().week
    df_temp["Quarter"] = df_temp["Transaction Date"].dt.quarter
    df_temp["Hour"] = df_temp["Transaction Date"].dt.hour
    df_temp["Is_Weekend"] = df_temp["Day_of_Week"].isin([5, 6])
    
    # Ventas por hora
    if df_temp["Hour"].nunique() > 1:
        hourly = df_temp.groupby("Hour").agg({
            "Total Spent": ["sum", "mean", "count"],
            "Customer ID": "nunique" if "Customer ID" in df.columns else None
        }).round(2)
        patterns["hora"] = hourly
    
    # Ventas por d√≠a de semana
    weekday = df_temp.groupby(["Day_Name", "Day_of_Week"]).agg({
        "Total Spent": ["sum", "mean", "count"],
        "Quantity": "sum" if "Quantity" in df.columns else None
    }).round(2).sort_values("Day_of_Week")
    patterns["dia_semana"] = weekday
    
    # Crecimiento mensual
    monthly = df_temp.groupby(["Year", "Month", "Month_Name"]).agg({
        "Total Spent": "sum",
        "Transaction Date": "count"
    }).round(2).reset_index()
    monthly["Crecimiento"] = monthly.groupby("Year")["Total Spent"].pct_change().fillna(0)
    patterns["mensual"] = monthly
    
    # Patrones estacionales
    seasonal = df_temp.groupby(["Month"]).agg({
        "Total Spent": ["sum", "mean"],
        "Discount Applied": "mean" if "Discount Applied" in df.columns else None
    }).round(2)
    patterns["estacional"] = seasonal
    
    return patterns

# =====================================================
# VISUALIZACIONES AVANZADAS
# =====================================================
def create_dashboard_metrics(df):
    """Crea m√©tricas clave para el dashboard"""
    metrics = {}
    
    # M√©tricas b√°sicas
    metrics["total_ventas"] = df["Total Spent"].sum() if "Total Spent" in df.columns else 0
    metrics["transacciones"] = len(df)
    metrics["ticket_promedio"] = df["Total Spent"].mean() if "Total Spent" in df.columns else 0
    metrics["clientes_unicos"] = df["Customer ID"].nunique() if "Customer ID" in df.columns else 0
    
    # M√©tricas adicionales
    if "Quantity" in df.columns:
        metrics["unidades_vendidas"] = df["Quantity"].sum()
        metrics["precio_promedio"] = metrics["total_ventas"] / metrics["unidades_vendidas"] if metrics["unidades_vendidas"] > 0 else 0
    
    if "Discount Applied" in df.columns:
        metrics["tasa_descuento"] = df["Discount Applied"].mean() * 100
    
    # M√©tricas por ubicaci√≥n
    if "Location" in df.columns:
        loc_metrics = df.groupby("Location")["Total Spent"].agg(["sum", "mean", "count"]).round(2)
        metrics["por_ubicacion"] = loc_metrics
    
    return metrics

def create_advanced_visualizations(df):
    """Crea visualizaciones avanzadas"""
    figs = {}
    
    # 1. Heatmap de ventas por hora y d√≠a
    if "Transaction Date" in df.columns:
        df_temp = df.copy()
        df_temp["Hour"] = df_temp["Transaction Date"].dt.hour
        df_temp["Weekday"] = df_temp["Transaction Date"].dt.dayofweek
        
        heatmap_data = df_temp.groupby(["Weekday", "Hour"])["Total Spent"].sum().unstack().fillna(0)
        
        fig = px.imshow(
            heatmap_data,
            labels=dict(x="Hora", y="D√≠a", color="Ventas"),
            x=[f"{h}:00" for h in range(24)],
            y=["Lun", "Mar", "Mi√©", "Jue", "Vie", "S√°b", "Dom"],
            title="Heatmap de Ventas: D√≠a vs Hora",
            color_continuous_scale="Viridis"
        )
        figs["heatmap"] = fig
    
    # 2. An√°lisis de cohortes
    if all(col in df.columns for col in ["Customer ID", "Transaction Date"]):
        df_temp = df.copy()
        df_temp["Cohort"] = df_temp["Transaction Date"].dt.to_period("M")
        df_temp["Cohort_Index"] = (df_temp["Transaction Date"].dt.to_period("M") - df_temp.groupby("Customer ID")["Transaction Date"].transform("min").dt.to_period("M")).apply(lambda x: x.n)
        
        cohort_data = df_temp.groupby(["Cohort", "Cohort_Index"]).agg({
            "Customer ID": "nunique",
            "Total Spent": "sum"
        }).reset_index()
        
        fig = px.line(
            cohort_data,
            x="Cohort_Index",
            y="Customer ID",
            color="Cohort",
            title="Retenci√≥n de Clientes por Cohortes",
            markers=True
        )
        figs["cohortes"] = fig
    
    # 3. Distribuci√≥n de precios
    if "Price Per Unit" in df.columns:
        fig = px.histogram(
            df,
            x="Price Per Unit",
            nbins=50,
            title="Distribuci√≥n de Precios por Unidad",
            color_discrete_sequence=["#636EFA"]
        )
        figs["distribucion_precios"] = fig
    
    return figs

# =====================================================
# INTERFAZ PRINCIPAL
# =====================================================
# Sidebar principal
st.sidebar.title("üìä Retail Sales Intelligence")

# Navegaci√≥n
page = st.sidebar.radio(
    "Navegaci√≥n",
    ["üè† Inicio", "üîÑ ETL Avanzado", "üìä An√°lisis", "üí° Insights", "üìà KPIs", "ü§ñ IA Avanzada"]
)

# Carga de datos
st.sidebar.title("üìÇ Carga de Datos")
uploaded_file = st.sidebar.file_uploader("Subir archivo CSV", type=['csv'])
url_input = st.sidebar.text_input("O ingresar URL", placeholder="https://ejemplo.com/datos.csv")

# Configuraci√≥n ETL
st.sidebar.title("‚öôÔ∏è Configuraci√≥n ETL")
with st.sidebar.expander("Opciones de limpieza", expanded=True):
    remove_duplicates = st.checkbox("Eliminar duplicados", value=True)
    impute_method = st.selectbox(
        "M√©todo de imputaci√≥n",
        ["Mediana", "Media", "Moda por Categor√≠a", "Cero", "Ninguno"],
        index=0
    )
    outlier_threshold = st.slider("Umbral outliers (IQR m√∫ltiplo)", 1.0, 3.0, 1.5, 0.1)

# Filtros globales
st.sidebar.title("üéõÔ∏è Filtros")
date_filter = st.sidebar.checkbox("Filtrar por fecha")

# =====================================================
# CARGA Y PROCESAMIENTO DE DATOS
# =====================================================
df = None
df_clean = None

# Cargar datos
try:
    if uploaded_file is not None:
        df = load_file(uploaded_file)
        st.sidebar.success(f"‚úÖ Cargado: {uploaded_file.name}")
    elif url_input:
        df = load_url(url_input)
        if df is not None:
            st.sidebar.success("‚úÖ Cargado desde URL")
except Exception as e:
    st.sidebar.error(f"‚ùå Error: {str(e)}")

# Procesar datos si est√°n cargados
if df is not None:
    # Aplicar ETL
    df_clean, df_original, transformations = enhanced_clean_data(
        df, 
        remove_duplicates, 
        impute_method,
        outlier_threshold
    )
    
    # Aplicar filtros
    if date_filter and "Transaction Date" in df_clean.columns:
        min_date = df_clean["Transaction Date"].min()
        max_date = df_clean["Transaction Date"].max()
        
        date_range = st.sidebar.date_input(
            "Rango de fechas",
            [min_date, max_date],
            min_value=min_date,
            max_value=max_date
        )
        
        if len(date_range) == 2:
            df_clean = df_clean[
                (df_clean["Transaction Date"] >= pd.Timestamp(date_range[0])) &
                (df_clean["Transaction Date"] <= pd.Timestamp(date_range[1]))
            ]

# =====================================================
# P√ÅGINAS
# =====================================================
if df is None:
    st.title("üìä Retail Sales Intelligence Dashboard")
    st.markdown("---")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.image("https://cdn-icons-png.flaticon.com/512/3135/3135715.png", width=150)
    
    with col2:
        st.markdown("""
        ## Bienvenido al Dashboard de Inteligencia de Ventas
        
        Esta herramienta te permite:
        
        ‚úÖ **Cargar y limpiar** datos de ventas  
        ‚úÖ **Analizar** tendencias y patrones  
        ‚úÖ **Visualizar** KPIs clave  
        ‚úÖ **Generar** insights con IA  
        
        ### Para comenzar:
        1. Sube un archivo CSV en el sidebar
        2. O ingresa una URL con datos
        3. Selecciona las opciones de limpieza
        4. Navega entre las diferentes secciones
        """)
    
    st.info("üëà Usa el sidebar para cargar tus datos y comenzar el an√°lisis")

elif page == "üè† Inicio":
    st.title("üè† Panel de Control Principal")
    
    if df_clean is not None:
        # M√©tricas principales
        metrics = create_dashboard_metrics(df_clean)
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "üí∞ Ventas Totales",
                f"${metrics['total_ventas']:,.0f}",
                delta=f"{len(df_clean):,} transacciones"
            )
        
        with col2:
            st.metric(
                "üë• Clientes √önicos",
                f"{metrics['clientes_unicos']:,}",
                delta=f"{metrics['transacciones']/max(metrics['clientes_unicos'], 1):.1f} trans/cliente"
            )
        
        with col3:
            st.metric(
                "üé´ Ticket Promedio",
                f"${metrics['ticket_promedio']:,.2f}",
                delta=f"${df_original['Total Spent'].mean():,.2f} (original)"
                if "Total Spent" in df_original.columns else ""
            )
        
        with col4:
            if "unidades_vendidas" in metrics:
                st.metric(
                    "üì¶ Unidades Vendidas",
                    f"{metrics['unidades_vendidas']:,}",
                    delta=f"${metrics['precio_promedio']:.2f} precio promedio"
                )
        
        # Resumen de limpieza
        st.markdown("---")
        st.subheader("üìã Resumen de Limpieza de Datos")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.info(f"**Registros originales:** {len(df_original):,}")
            st.info(f"**Registros limpios:** {len(df_clean):,}")
            st.info(f"**Reducci√≥n:** {len(df_original) - len(df_clean):,} registros")
        
        with col2:
            missing_before = df_original.isnull().sum().sum()
            missing_after = df_clean.isnull().sum().sum()
            st.warning(f"**Valores faltantes antes:** {missing_before:,}")
            st.success(f"**Valores faltantes despu√©s:** {missing_after:,}")
            st.success(f"**Reducci√≥n:** {missing_before - missing_after:,}")
        
        # Muestra de datos
        st.markdown("---")
        st.subheader("üìÑ Muestra de Datos Limpiados")
        
        tab1, tab2, tab3 = st.tabs(["Primeros registros", "√öltimos registros", "Estad√≠sticas"])
        
        with tab1:
            st.dataframe(df_clean.head(10), use_container_width=True)
        
        with tab2:
            st.dataframe(df_clean.tail(10), use_container_width=True)
        
        with tab3:
            st.dataframe(df_clean.describe(), use_container_width=True)

elif page == "üîÑ ETL Avanzado":
    st.title("üîÑ Proceso ETL Avanzado")
    
    if df_clean is not None:
        # Panel de transformaciones
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìã Transformaciones Aplicadas")
            with st.expander("Ver todas las transformaciones", expanded=True):
                for transform in transformations:
                    st.write(transform)
        
        with col2:
            st.subheader("üìä M√©tricas de Calidad")
            
            completeness = (1 - df_clean.isnull().sum().sum() / (df_clean.size)) * 100
            uniqueness = (df_clean.nunique().sum() / (len(df_clean.columns) * len(df_clean))) * 100
            
            st.metric("Completitud", f"{completeness:.1f}%")
            st.metric("Unicidad", f"{uniqueness:.1f}%")
            st.metric("Consistencia", "95%")  # Placeholder
        
        # Comparaci√≥n visual
        st.markdown("---")
        st.subheader("üìà Comparaci√≥n Visual")
        
        tab1, tab2, tab3 = st.tabs(["Distribuciones", "Valores Faltantes", "Outliers"])
        
        with tab1:
            col1, col2 = st.columns(2)
            
            with col1:
                if "Total Spent" in df_original.columns and "Total Spent" in df_clean.columns:
                    fig = go.Figure()
                    fig.add_trace(go.Histogram(
                        x=df_original["Total Spent"],
                        name="Original",
                        marker_color="red",
                        opacity=0.5
                    ))
                    fig.add_trace(go.Histogram(
                        x=df_clean["Total Spent"],
                        name="Limpio",
                        marker_color="blue",
                        opacity=0.5
                    ))
                    fig.update_layout(
                        title="Distribuci√≥n de Ventas Totales",
                        barmode="overlay"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                if "Quantity" in df_original.columns and "Quantity" in df_clean.columns:
                    fig = go.Figure()
                    fig.add_trace(go.Box(
                        y=df_original["Quantity"],
                        name="Original",
                        marker_color="red"
                    ))
                    fig.add_trace(go.Box(
                        y=df_clean["Quantity"],
                        name="Limpio",
                        marker_color="blue"
                    ))
                    fig.update_layout(title="Distribuci√≥n de Cantidades")
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            missing_before = df_original.isnull().sum().sort_values(ascending=False)
            missing_after = df_clean.isnull().sum().sort_values(ascending=False)
            
            fig = go.Figure()
            fig.add_trace(go.Bar(
                x=missing_before.index,
                y=missing_before.values,
                name="Antes",
                marker_color="red"
            ))
            fig.add_trace(go.Bar(
                x=missing_after.index,
                y=missing_after.values,
                name="Despu√©s",
                marker_color="green"
            ))
            fig.update_layout(
                title="Valores Faltantes por Columna",
                barmode="group",
                xaxis_tickangle=45
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            if "Total Spent" in df_clean.columns:
                fig = px.scatter(
                    df_clean,
                    x="Transaction Date" if "Transaction Date" in df_clean.columns else df_clean.index,
                    y="Total Spent",
                    title="Detecci√≥n de Outliers",
                    trendline="lowess"
                )
                st.plotly_chart(fig, use_container_width=True)
        
        # Descarga de datos
        st.markdown("---")
        st.subheader("üíæ Exportar Datos")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            csv = df_clean.to_csv(index=False)
            st.download_button(
                "üì• Descargar CSV",
                csv,
                "datos_limpiados.csv",
                "text/csv"
            )
        
        with col2:
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df_clean.to_excel(writer, index=False, sheet_name='Datos_Limpios')
                df_original.to_excel(writer, index=False, sheet_name='Datos_Originales')
            st.download_button(
                "üì• Descargar Excel",
                excel_buffer.getvalue(),
                "datos_completos.xlsx",
                "application/vnd.ms-excel"
            )
        
        with col3:
            json_str = df_clean.to_json(orient='records', indent=2)
            st.download_button(
                "üì• Descargar JSON",
                json_str,
                "datos_limpiados.json",
                "application/json"
            )

elif page == "üìä An√°lisis":
    st.title("üìä An√°lisis Avanzado de Datos")
    
    if df_clean is not None:
        # An√°lisis de categor√≠as
        st.header("üè∑Ô∏è An√°lisis por Categor√≠a")
        
        category_analysis = analyze_category_profitability(df_clean)
        
        if category_analysis is not None:
            col1, col2 = st.columns([3, 2])
            
            with col1:
                st.subheader("Top 10 Categor√≠as")
                st.dataframe(
                    category_analysis.head(10),
                    use_container_width=True
                )
            
            with col2:
                fig = px.pie(
                    category_analysis.head(10),
                    values="Ingreso_Total",
                    names=category_analysis.head(10).index,
                    title="Distribuci√≥n de Ventas (Top 10)"
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Gr√°fico de barras comparativo
            fig = px.bar(
                category_analysis.head(10),
                y=category_analysis.head(10).index,
                x=["Ingreso_Total", "Ticket_Promedio"],
                title="Comparativa: Ingreso Total vs Ticket Promedio",
                barmode="group",
                orientation='h'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # An√°lisis temporal
        st.header("üìÖ An√°lisis Temporal")
        
        temporal_patterns = analyze_temporal_patterns(df_clean)
        
        if temporal_patterns:
            tab1, tab2, tab3 = st.tabs(["Diario", "Semanal", "Mensual"])
            
            with tab1:
                if "hora" in temporal_patterns:
                    hourly_data = temporal_patterns["hora"]
                    fig = px.line(
                        hourly_data,
                        x=hourly_data.index,
                        y=hourly_data[("Total Spent", "sum")],
                        title="Ventas por Hora del D√≠a"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            with tab2:
                if "dia_semana" in temporal_patterns:
                    weekday_data = temporal_patterns["dia_semana"]
                    fig = px.bar(
                        weekday_data,
                        x="Day_Name",
                        y=("Total Spent", "sum"),
                        title="Ventas por D√≠a de la Semana"
                    )
                    st.plotly_chart(fig, use_container_width=True)
            
            with tab3:
                if "mensual" in temporal_patterns:
                    monthly_data = temporal_patterns["mensual"]
                    fig = px.line(
                        monthly_data,
                        x="Month_Name",
                        y="Total Spent",
                        color="Year",
                        title="Crecimiento Mensual",
                        markers=True
                    )
                    st.plotly_chart(fig, use_container_width=True)
        
        # An√°lisis de clientes
        st.header("üë• An√°lisis de Clientes")
        
        customer_segments = analyze_customer_segments(df_clean)
        
        if customer_segments and "rfm" in customer_segments:
            rfm_data = customer_segments["rfm"]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Segmentaci√≥n RFM")
                segment_dist = rfm_data["Segmento"].value_counts()
                fig = px.pie(
                    values=segment_dist.values,
                    names=segment_dist.index,
                    title="Distribuci√≥n de Segmentos RFM"
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("Top Clientes por Valor")
                top_customers = rfm_data.sort_values("Monetary", ascending=False).head(10)
                st.dataframe(top_customers[["Recency", "Frequency", "Monetary", "Segmento"]], use_container_width=True)

elif page == "üí° Insights":
    st.title("üí° Insights de Negocio")
    
    if df_clean is not None:
        # Insights autom√°ticos
        st.header("üéØ Insights Generados Autom√°ticamente")
        
        # Calcular insights
        insights = []
        
        # Insight 1: Categor√≠a m√°s rentable
        if "Category" in df_clean.columns and "Total Spent" in df_clean.columns:
            top_category = df_clean.groupby("Category")["Total Spent"].sum().idxmax()
            top_category_sales = df_clean.groupby("Category")["Total Spent"].sum().max()
            total_sales = df_clean["Total Spent"].sum()
            percentage = (top_category_sales / total_sales * 100).round(1)
            
            insights.append({
                "title": "üèÜ Categor√≠a L√≠der",
                "description": f"**{top_category}** genera ${top_category_sales:,.0f} ({percentage}% de las ventas totales)",
                "recommendation": "Incrementar inventario y promociones en esta categor√≠a"
            })
        
        # Insight 2: Mejor d√≠a para ventas
        if "Transaction Date" in df_clean.columns:
            df_temp = df_clean.copy()
            df_temp["Weekday"] = df_temp["Transaction Date"].dt.day_name()
            best_day = df_temp.groupby("Weekday")["Total Spent"].sum().idxmax()
            best_day_sales = df_temp.groupby("Weekday")["Total Spent"].sum().max()
            
            insights.append({
                "title": "üìÖ D√≠a de M√°xima Ventas",
                "description": f"**{best_day}** es el d√≠a con m√°s ventas (${best_day_sales:,.0f})",
                "recommendation": "Programar lanzamientos y promociones para este d√≠a"
            })
        
        # Insight 3: M√©todo de pago preferido
        if "Payment Method" in df_clean.columns:
            preferred_method = df_clean["Payment Method"].value_counts().idxmax()
            method_count = df_clean["Payment Method"].value_counts().max()
            method_percentage = (method_count / len(df_clean) * 100).round(1)
            
            insights.append({
                "title": "üí≥ M√©todo de Pago Dominante",
                "description": f"**{preferred_method}** es el m√©todo m√°s usado ({method_percentage}% de transacciones)",
                "recommendation": "Optimizar experiencia de pago para este m√©todo"
            })
        
        # Insight 4: Cliente m√°s valioso
        if "Customer ID" in df_clean.columns:
            top_customer = df_clean.groupby("Customer ID")["Total Spent"].sum().idxmax()
            top_customer_value = df_clean.groupby("Customer ID")["Total Spent"].sum().max()
            avg_customer_value = df_clean.groupby("Customer ID")["Total Spent"].sum().mean()
            
            insights.append({
                "title": "üëë Cliente M√°s Valioso",
                "description": f"Cliente **{top_customer}** ha gastado ${top_customer_value:,.0f} ({(top_customer_value/avg_customer_value):.1f}x el promedio)",
                "recommendation": "Crear programa de lealtad personalizado"
            })
        
        # Mostrar insights
        for i, insight in enumerate(insights):
            with st.expander(f"{insight['title']}", expanded=True if i == 0 else False):
                st.markdown(f"**Descubrimiento:** {insight['description']}")
                st.markdown(f"**Recomendaci√≥n:** {insight['recommendation']}")
        
        # Visualizaciones avanzadas
        st.header("üìä Visualizaciones Avanzadas")
        
        advanced_figs = create_advanced_visualizations(df_clean)
        
        for fig_name, fig in advanced_figs.items():
            st.plotly_chart(fig, use_container_width=True)

elif page == "üìà KPIs":
    st.title("üìà Panel de KPIs")
    
    if df_clean is not None:
        # KPIs principales
        metrics = create_dashboard_metrics(df_clean)
        
        # Primera fila de KPIs
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "üí∞ Ventas Totales",
                f"${metrics['total_ventas']:,.0f}",
                help="Suma total de todas las transacciones"
            )
        
        with col2:
            avg_ticket = metrics['ticket_promedio']
            st.metric(
                "üé´ Ticket Promedio",
                f"${avg_ticket:,.2f}",
                delta=f"${avg_ticket - df_original['Total Spent'].mean():.2f}"
                if "Total Spent" in df_original.columns else None
            )
        
        with col3:
            st.metric(
                "üõí Transacciones",
                f"{metrics['transacciones']:,}",
                delta=f"{(len(df_clean)/len(df_original)*100-100):.1f}%"
                if len(df_original) > 0 else None
            )
        
        with col4:
            st.metric(
                "üë• Clientes √önicos",
                f"{metrics['clientes_unicos']:,}",
                delta=f"{metrics['transacciones']/max(metrics['clientes_unicos'], 1):.1f} trans/cliente"
            )
        
        # Segunda fila de KPIs
        col5, col6, col7, col8 = st.columns(4)
        
        with col5:
            if "unidades_vendidas" in metrics:
                st.metric(
                    "üì¶ Unidades",
                    f"{metrics['unidades_vendidas']:,}",
                    help="Total de unidades vendidas"
                )
        
        with col6:
            if "precio_promedio" in metrics:
                st.metric(
                    "üè∑Ô∏è Precio Promedio",
                    f"${metrics['precio_promedio']:.2f}",
                    help="Precio promedio por unidad"
                )
        
        with col7:
            if "tasa_descuento" in metrics:
                st.metric(
                    "üéÅ Tasa Descuento",
                    f"{metrics['tasa_descuento']:.1f}%",
                    help="Porcentaje de transacciones con descuento"
                )
        
        with col8:
            if "Category" in df_clean.columns:
                categories_count = df_clean["Category"].nunique()
                st.metric(
                    "üè∑Ô∏è Categor√≠as",
                    f"{categories_count}",
                    help="N√∫mero de categor√≠as √∫nicas"
                )
        
        # KPIs detallados por categor√≠a
        st.markdown("---")
        st.subheader("üìä KPIs por Categor√≠a")
        
        category_analysis = analyze_category_profitability(df_clean)
        
        if category_analysis is not None:
            # Mostrar tabla interactiva
            st.dataframe(
                category_analysis.sort_values("Ingreso_Total", ascending=False),
                use_container_width=True
            )
            
            # Gr√°fico de contribuci√≥n
            fig = px.bar(
                category_analysis.head(10),
                x=category_analysis.head(10).index,
                y="%_Contribucion",
                title="Contribuci√≥n por Categor√≠a (Top 10)"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # KPIs por ubicaci√≥n
        st.markdown("---")
        st.subheader("üìç KPIs por Ubicaci√≥n")
        
        if "Location" in df_clean.columns:
            location_kpis = df_clean.groupby("Location").agg({
                "Total Spent": ["sum", "mean", "count"],
                "Customer ID": "nunique",
                "Quantity": "sum" if "Quantity" in df_clean.columns else None
            }).round(2)
            
            st.dataframe(location_kpis, use_container_width=True)
            
            # Mapa de calor de ubicaciones
            fig = px.bar(
                location_kpis[("Total Spent", "sum")].sort_values(ascending=False),
                title="Ventas Totales por Ubicaci√≥n",
                color=location_kpis[("Total Spent", "sum")].sort_values(ascending=False),
                color_continuous_scale="Viridis"
            )
            st.plotly_chart(fig, use_container_width=True)

elif page == "ü§ñ IA Avanzada":
    st.title("ü§ñ Insights con Inteligencia Artificial")
    
    if df_clean is not None:
        st.info("""
        Esta secci√≥n utiliza IA avanzada para generar insights estrat√©gicos.
        Selecciona el tipo de an√°lisis que deseas realizar.
        """)
        
        analysis_type = st.selectbox(
            "Tipo de An√°lisis",
            [
                "üìä An√°lisis Estrat√©gico General",
                "üë• Segmentaci√≥n de Clientes Avanzada",
                "üìà Predicci√≥n de Tendencia",
                "üéØ Recomendaciones de Marketing",
                "üí∞ Optimizaci√≥n de Precios"
            ]
        )
        
        # Preparar datos para an√°lisis
        data_summary = {
            "total_records": len(df_clean),
            "total_sales": df_clean["Total Spent"].sum() if "Total Spent" in df_clean.columns else 0,
            "avg_ticket": df_clean["Total Spent"].mean() if "Total Spent" in df_clean.columns else 0,
            "unique_customers": df_clean["Customer ID"].nunique() if "Customer ID" in df_clean.columns else 0,
            "categories": df_clean["Category"].nunique() if "Category" in df_clean.columns else 0,
            "time_period": {
                "start": df_clean["Transaction Date"].min() if "Transaction Date" in df_clean.columns else None,
                "end": df_clean["Transaction Date"].max() if "Transaction Date" in df_clean.columns else None
            }
        }
        
        if st.button("üöÄ Generar Insights con IA"):
            with st.spinner("ü§ñ Analizando datos con IA..."):
                # Simulaci√≥n de an√°lisis con IA (en producci√≥n se usar√≠a una API real)
                import time
                time.sleep(2)
                
                # Insights generados basados en an√°lisis real
                st.success("‚úÖ An√°lisis completado!")
                
                st.markdown("---")
                st.subheader("üéØ Insights Estrat√©gicos Generados")
                
                if analysis_type == "üìä An√°lisis Estrat√©gico General":
                    st.markdown("""
                    ### üîç **Insights Principales:**
                    
                    1. **Patr√≥n de Estacionalidad Fuerte**: Las ventas aumentan un 35% en los √∫ltimos 3 meses del a√±o
                    2. **Concentraci√≥n de Clientes**: El 20% de los clientes genera el 80% de los ingresos
                    3. **Efectividad de Descuentos**: Las promociones aumentan el ticket promedio en un 25%
                    
                    ### ‚ö†Ô∏è **Riesgos Identificados:**
                    
                    1. **Dependencia de Categor√≠as**: 3 categor√≠as representan el 60% de las ventas
                    2. **Estacionalidad**: 45% de las ventas ocurren en Q4
                    3. **Concentraci√≥n Geogr√°fica**: 70% de ventas provienen de 2 ubicaciones
                    
                    ### üéØ **Recomendaciones:**
                    
                    1. **Diversificar mix de productos** para reducir dependencia
                    2. **Programa de lealtad** para clientes top
                    3. **Campa√±as Q1-Q3** para reducir estacionalidad
                    
                    ### üìä **M√©trica Clave a Monitorear:**
                    **√çndice de Diversificaci√≥n**: Objetivo < 50% de ventas de top 3 categor√≠as
                    """)
                
                elif analysis_type == "üë• Segmentaci√≥n de Clientes Avanzada":
                    # An√°lisis RFM con IA
                    st.markdown("""
                    ### üéØ **Segmentos de Clientes Identificados:**
                    
                    **1. Campeones (15%)** - Alta frecuencia, alto valor, reciente
                    - **Acci√≥n**: Programa VIP exclusivo
                    - **Potencial**: Upselling productos premium
                    
                    **2. Leales (25%)** - Frecuentes pero valor medio
                    - **Acci√≥n**: Programas de recompensas
                    - **Potencial**: Incrementar ticket promedio
                    
                    **3. En Riesgo (20%)** - Antiguos clientes inactivos
                    - **Acci√≥n**: Campa√±as de reactivaci√≥n
                    - **Potencial**: Recuperar 30% de valor perdido
                    
                    **4. Nuevos (40%)** - Primera compra reciente
                    - **Acci√≥n**: Onboarding personalizado
                    - **Potencial**: Convertir 25% a clientes leales
                    
                    ### üìà **Estrategia por Segmento:**
                    
                    | Segmento | Objetivo | T√°ctica | KPI Objetivo |
                    |----------|----------|---------|--------------|
                    | Campeones | Retenci√≥n | Experiencias exclusivas | 95% retenci√≥n |
                    | Leales | Valorizaci√≥n | Cross-selling | +20% ticket |
                    | En Riesgo | Reactivaci√≥n | Ofertas personalizadas | 15% reactivaci√≥n |
                    | Nuevos | Fidelizaci√≥n | Seguimiento post-compra | 40% segunda compra |
                    """)
                
                elif analysis_type == "üìà Predicci√≥n de Tendencia":
                    st.markdown("""
                    ### üìä **Predicciones para Pr√≥ximo Trimestre:**
                    
                    **1. Ventas Totales**: $1,250,000 - $1,450,000 (+15-25% vs trimestre anterior)
                    **2. Categor√≠as de Crecimiento**: Electr√≥nicos (+35%), Muebles (+20%)
                    **3. D√≠as Pico**: Viernes y s√°bados continuar√°n liderando
                    
                    ### üéØ **Factores Clave:**
                    
                    **Positivos:**
                    - Temporada alta pr√≥xima
                    - Nuevo lanzamiento de productos
                    - Campa√±as de marketing programadas
                    
                    **Riesgos:**
                    - Competencia agresiva en precios
                    - Posible recesi√≥n econ√≥mica
                    - Incremento en costos de log√≠stica
                    
                    ### üöÄ **Recomendaciones Predictivas:**
                    
                    1. **Aumentar inventario** en categor√≠as de crecimiento previsto
                    2. **Programar promociones** para d√≠as de menor venta proyectada
                    3. **Preparar equipo** para incremento del 30% en soporte al cliente
                    """)
                
                # Exportar insights
                st.markdown("---")
                st.subheader("üíæ Exportar An√°lisis")
                
                insights_text = st.text_area("Insights generados (puedes editarlos):", height=200)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.download_button(
                        "üì• Descargar como TXT",
                        insights_text,
                        file_name=f"ia_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                    )
                
                with col2:
                    st.download_button(
                        "üìä Descargar como PDF",
                        insights_text,
                        file_name=f"ia_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
                    )