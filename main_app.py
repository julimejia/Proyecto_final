# =====================================================
# IMPORTS & CONFIG
# =====================================================
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import requests
import io
from datetime import datetime
import calendar

st.set_page_config(page_title="Retail Sales Intelligent Dashboard", layout="wide")

# =====================================================
# CACHE FUNCTIONS
# =====================================================
@st.cache_data
def load_file(file):
    return pd.read_csv(file)

@st.cache_data
def load_url(url):
    response = requests.get(url)
    return pd.read_csv(io.StringIO(response.text))

# =====================================================
# CLEANING LOGIC (MEJORADA)
# =====================================================
def clean_data(df, remove_duplicates, impute_method, outlier_threshold):
    df_original = df.copy()
    
    # Registrar transformaciones
    transformations = []
    
    # Eliminar duplicados
    if remove_duplicates:
        duplicates_before = df.shape[0]
        df = df.drop_duplicates()
        duplicates_removed = duplicates_before - df.shape[0]
        if duplicates_removed > 0:
            transformations.append(f"üìä Se removieron {duplicates_removed} filas duplicadas")
    
    # Convertir fecha
    if "Transaction Date" in df.columns:
        df["Transaction Date"] = pd.to_datetime(df["Transaction Date"], errors="coerce")
        transformations.append("üìÖ 'Transaction Date' convertida a datetime")
    
    # Limpieza especial para 'Discount Applied'
    if "Discount Applied" in df.columns:
        # Convertir a booleano manejando diferentes formatos
        discount_values = df["Discount Applied"].astype(str).str.lower().str.strip()
        
        # Mapeo de valores comunes a booleano
        discount_mapping = {
            'true': True, 'yes': True, '1': True, 'verdadero': True, 'si': True,
            'false': False, 'no': False, '0': False, 'falso': False
        }
        
        df["Discount Applied"] = discount_values.map(discount_mapping)
        null_before = df_original["Discount Applied"].isnull().sum()
        null_after = df["Discount Applied"].isnull().sum()
        
        if null_before > null_after:
            transformations.append(f"‚úÖ 'Discount Applied' limpiado: {null_before - null_after} valores convertidos")
    
    # Imputaci√≥n para columnas num√©ricas
    numeric_cols = df.select_dtypes(include=np.number).columns
    
    if len(numeric_cols) > 0 and impute_method != "Ninguna":
        missing_before = df[numeric_cols].isnull().sum().sum()
        
        if impute_method == "Media":
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
        elif impute_method == "Mediana":
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        elif impute_method == "Cero":
            df[numeric_cols] = df[numeric_cols].fillna(0)
        
        missing_after = df[numeric_cols].isnull().sum().sum()
        if missing_before > missing_after:
            transformations.append(f"üîß Imputaci√≥n ({impute_method}): {missing_before - missing_after} valores")
    
    # Manejo de outliers
    outliers_removed = 0
    for col in numeric_cols:
        mean = df[col].mean()
        std = df[col].std()
        if std > 0:  # Evitar divisi√≥n por cero
            upper = mean + outlier_threshold * std
            lower = mean - outlier_threshold * std
            outliers = ((df[col] > upper) | (df[col] < lower)).sum()
            outliers_removed += outliers
            df = df[(df[col] <= upper) & (df[col] >= lower)]
    
    if outliers_removed > 0:
        transformations.append(f"üìà Outliers removidos: {outliers_removed} (threshold: {outlier_threshold}œÉ)")
    
    return df, df_original, transformations

# =====================================================
# ANALYSIS FUNCTIONS
# =====================================================
def analyze_category_profitability(df):
    """Analiza rentabilidad por categor√≠a"""
    if "Category" not in df.columns or "Total Spent" not in df.columns:
        return None
    
    # Calcular m√©tricas por categor√≠a
    category_analysis = df.groupby("Category").agg({
        "Total Spent": ["sum", "mean", "count"],
        "Quantity": "sum" if "Quantity" in df.columns else "count"
    }).round(2)
    
    # Aplanar columnas multi-index
    category_analysis.columns = ['_'.join(col).strip() for col in category_analysis.columns.values]
    
    # Renombrar para claridad
    category_analysis = category_analysis.rename(columns={
        "Total Spent_sum": "Ingreso_Total",
        "Total Spent_mean": "Ticket_Promedio",
        "Total Spent_count": "Transacciones",
        "Quantity_sum": "Cantidad_Total" if "Quantity" in df.columns else "Transacciones"
    })
    
    # Calcular porcentaje de contribuci√≥n
    category_analysis["%_Contribuci√≥n"] = (category_analysis["Ingreso_Total"] / 
                                         category_analysis["Ingreso_Total"].sum() * 100).round(2)
    
    # Ordenar por rentabilidad
    category_analysis = category_analysis.sort_values("Ingreso_Total", ascending=False)
    
    return category_analysis

def analyze_customer_segments(df):
    """Analiza segmentos de clientes"""
    segments = {}
    
    # An√°lisis por ubicaci√≥n
    if "Location" in df.columns and "Total Spent" in df.columns:
        location_analysis = df.groupby("Location").agg({
            "Total Spent": ["sum", "mean", "count"],
            "Customer ID": "nunique" if "Customer ID" in df.columns else None
        }).round(2)
        
        if location_analysis.isnull().all().all():
            location_analysis = None
        else:
            segments["ubicacion"] = location_analysis
    
    # An√°lisis por m√©todo de pago
    if "Payment Method" in df.columns and "Total Spent" in df.columns:
        payment_analysis = df.groupby("Payment Method").agg({
            "Total Spent": ["sum", "mean", "count"]
        }).round(2)
        segments["metodo_pago"] = payment_analysis
    
    # An√°lisis por categor√≠a preferida del cliente
    if "Customer ID" in df.columns and "Category" in df.columns:
        customer_category = df.groupby(["Customer ID", "Category"])["Total Spent"].sum().reset_index()
        top_category_per_customer = customer_category.loc[
            customer_category.groupby("Customer ID")["Total Spent"].idxmax()
        ]
        segments["categoria_preferida"] = top_category_per_customer["Category"].value_counts()
    
    return segments

def analyze_temporal_patterns(df):
    """Analiza patrones temporales"""
    if "Transaction Date" not in df.columns:
        return None
    
    patterns = {}
    
    # Crear columnas temporales
    df_temp = df.copy()
    df_temp["Year"] = df_temp["Transaction Date"].dt.year
    df_temp["Month"] = df_temp["Transaction Date"].dt.month
    df_temp["Month_Name"] = df_temp["Transaction Date"].dt.strftime('%B')
    df_temp["Day"] = df_temp["Transaction Date"].dt.day
    df_temp["Day_of_Week"] = df_temp["Transaction Date"].dt.dayofweek
    df_temp["Day_Name"] = df_temp["Transaction Date"].dt.strftime('%A')
    df_temp["Week"] = df_temp["Transaction Date"].dt.isocalendar().week
    df_temp["Quarter"] = df_temp["Transaction Date"].dt.quarter
    
    # Ventas por d√≠a de la semana
    weekday_sales = df_temp.groupby(["Day_Name", "Day_of_Week"])["Total Spent"].agg(['sum', 'mean', 'count']).reset_index()
    weekday_sales = weekday_sales.sort_values("Day_of_Week")
    patterns["dia_semana"] = weekday_sales
    
    # Ventas por mes
    monthly_sales = df_temp.groupby(["Month_Name", "Month"])["Total Spent"].agg(['sum', 'mean', 'count']).reset_index()
    monthly_sales = monthly_sales.sort_values("Month")
    patterns["mes"] = monthly_sales
    
    # Ventas por trimestre
    quarterly_sales = df_temp.groupby("Quarter")["Total Spent"].agg(['sum', 'mean', 'count']).reset_index()
    patterns["trimestre"] = quarterly_sales
    
    # Ventas por hora (si hubiera)
    if df_temp["Transaction Date"].dt.hour.nunique() > 1:
        df_temp["Hour"] = df_temp["Transaction Date"].dt.hour
        hourly_sales = df_temp.groupby("Hour")["Total Spent"].agg(['sum', 'mean', 'count']).reset_index()
        patterns["hora"] = hourly_sales
    
    return patterns

# =====================================================
# SIDEBAR NAVIGATION
# =====================================================
st.sidebar.title("üìä Navigation")
page = st.sidebar.radio(
    "Go to",
    ["ETL", "EDA", "Business Insights", "KPIs", "AI Insights"]
)

st.sidebar.title("üìÇ Data Source")

uploaded_file = st.sidebar.file_uploader("Upload CSV", type=['csv'])
url_input = st.sidebar.text_input("Or URL", placeholder="https://example.com/data.csv")

# =====================================================
# DATA LOADING
# =====================================================
df = None

try:
    if uploaded_file:
        df = load_file(uploaded_file)
        st.sidebar.success(f"‚úÖ Loaded: {uploaded_file.name} ({len(df)} rows)")
    elif url_input and url_input.startswith(('http://', 'https://')):
        df = load_url(url_input)
        st.sidebar.success(f"‚úÖ Loaded from URL ({len(df)} rows)")
except Exception as e:
    st.sidebar.error(f"‚ùå Error loading data: {str(e)}")

if df is None:
    st.info("üëà Please upload a CSV file or enter a URL to begin")
    st.stop()

# =====================================================
# ETL CONTROLS
# =====================================================
st.sidebar.title("üîß ETL Controls")

remove_duplicates = st.sidebar.checkbox("Remove duplicates", value=True)

impute_method = st.sidebar.selectbox(
    "Imputation method",
    ["Ninguna", "Media", "Mediana", "Cero"],
    index=1
)

outlier_threshold = st.sidebar.slider("Outlier threshold (œÉ)", 1.0, 5.0, 3.0, 0.5)

df_clean, df_original, transformations = clean_data(df, remove_duplicates, impute_method, outlier_threshold)

# =====================================================
# GLOBAL FILTERS
# =====================================================
st.sidebar.title("üéõÔ∏è Global Filters")

if "Category" in df_clean.columns:
    categories = st.sidebar.multiselect(
        "Category",
        df_clean["Category"].dropna().unique(),
        default=df_clean["Category"].dropna().unique()
    )
    df_clean = df_clean[df_clean["Category"].isin(categories)]

if "Location" in df_clean.columns:
    locations = st.sidebar.multiselect(
        "Location",
        df_clean["Location"].dropna().unique(),
        default=df_clean["Location"].dropna().unique()
    )
    df_clean = df_clean[df_clean["Location"].isin(locations)]

# =====================================================
# ETL PAGE
# =====================================================
if page == "ETL":
    st.title("üîÑ ETL Interactive Dashboard")
    
    # Mostrar transformaciones realizadas
    if transformations:
        st.subheader("Transformaciones aplicadas")
        for transform in transformations:
            st.write(f"‚Ä¢ {transform}")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìã Before Cleaning")
        st.metric("Rows", df_original.shape[0])
        st.metric("Columns", df_original.shape[1])
        st.dataframe(df_original.head(), use_container_width=True)
        
        # Estad√≠sticas antes
        st.write("**Missing values before:**")
        st.dataframe(df_original.isnull().sum().to_frame("Count"), use_container_width=True)
    
    with col2:
        st.subheader("‚ú® After Cleaning")
        st.metric("Rows", df_clean.shape[0], delta=df_clean.shape[0]-df_original.shape[0])
        st.metric("Columns", df_clean.shape[1])
        st.dataframe(df_clean.head(), use_container_width=True)
        
        # Estad√≠sticas despu√©s
        st.write("**Missing values after:**")
        st.dataframe(df_clean.isnull().sum().to_frame("Count"), use_container_width=True)
    
    # Descarga de datos limpios
    st.download_button(
        "üíæ Download Clean CSV",
        df_clean.to_csv(index=False),
        "clean_data.csv",
        "text/csv"
    )

# =====================================================
# EDA PAGE
# =====================================================
elif page == "EDA":
    st.title("üîç Exploratory Data Analysis")
    
    # Secci√≥n 1: Informaci√≥n general
    with st.expander("üìä Informaci√≥n General", expanded=True):
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Primeras filas")
            st.dataframe(df_clean.head())
        
        with col2:
            st.subheader("Resumen")
            buffer = io.StringIO()
            df_clean.info(buf=buffer)
            st.text(buffer.getvalue())
        
        st.subheader("Estad√≠sticas descriptivas")
        st.dataframe(df_clean.describe())
    
    # Secci√≥n 2: Calidad de datos
    with st.expander("üßπ Calidad de Datos"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Valores Faltantes")
            missing_count = df_clean.isnull().sum()
            missing_percentage = (df_clean.isnull().sum() / len(df_clean)) * 100
            missing_data = pd.DataFrame({
                'Missing': missing_count,
                '%': missing_percentage.round(2)
            }).sort_values(by='Missing', ascending=False)
            
            missing_data = missing_data[missing_data['Missing'] > 0]
            if not missing_data.empty:
                st.dataframe(missing_data)
            else:
                st.success("‚úÖ No hay valores faltantes")
        
        with col2:
            st.subheader("Filas Duplicadas")
            duplicate_rows_count = df_clean.duplicated().sum()
            if duplicate_rows_count > 0:
                st.warning(f"‚ö†Ô∏è {duplicate_rows_count} filas duplicadas encontradas")
                st.dataframe(df_clean[df_clean.duplicated()].head())
            else:
                st.success("‚úÖ No hay filas duplicadas")
    
    # Secci√≥n 3: Visualizaciones
    with st.expander("üìà Visualizaciones"):
        numeric_cols = df_clean.select_dtypes(include=np.number).columns
        
        if len(numeric_cols) > 0:
            tab1, tab2, tab3 = st.tabs(["Univariado", "Bivariado", "Temporal"])
            
            with tab1:
                col = st.selectbox("Variable num√©rica", numeric_cols)
                fig = px.histogram(df_clean, x=col, title=f'Distribuci√≥n de {col}')
                st.plotly_chart(fig, use_container_width=True)
            
            with tab2:
                if len(numeric_cols) > 1:
                    col1 = st.selectbox("Variable X", numeric_cols, key="x")
                    col2 = st.selectbox("Variable Y", numeric_cols, key="y")
                    
                    # Opci√≥n para agregar color por categor√≠a
                    color_by = None
                    if "Category" in df_clean.columns:
                        color_by = st.checkbox("Color por categor√≠a")
                    
                    if color_by:
                        fig = px.scatter(df_clean, x=col1, y=col2, color="Category",
                                       title=f'{col1} vs {col2} por Categor√≠a')
                    else:
                        fig = px.scatter(df_clean, x=col1, y=col2, title=f'{col1} vs {col2}')
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            with tab3:
                if "Transaction Date" in df_clean.columns:
                    df_temp = df_clean.copy()
                    df_temp = df_temp.sort_values("Transaction Date")
                    
                    # Opciones de agregaci√≥n
                    col1, col2 = st.columns(2)
                    with col1:
                        freq = st.selectbox("Frecuencia", ["D", "W", "M", "Q", "Y"])
                    with col2:
                        metric = st.selectbox("M√©trica", ["sum", "mean", "count"])
                    
                    # Resample
                    ts_data = df_temp.set_index("Transaction Date")["Total Spent"]
                    ts_resampled = ts_data.resample(freq).agg(metric).reset_index()
                    
                    fig = px.line(ts_resampled, x="Transaction Date", y="Total Spent",
                                title=f'Ventas Totales ({metric})')
                    st.plotly_chart(fig, use_container_width=True)

# =====================================================
# BUSINESS INSIGHTS PAGE
# =====================================================
elif page == "Business Insights":
    st.title("üí° Business Insights Dashboard")
    
    # Pregunta 1: Rentabilidad por categor√≠a
    st.header("1. üìä An√°lisis de Rentabilidad por Categor√≠a")
    
    category_analysis = analyze_category_profitability(df_clean)
    
    if category_analysis is not None:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Top 5 Categor√≠as por Ingreso")
            top_categories = category_analysis.head().copy()
            st.dataframe(top_categories)
            
            # Gr√°fico de barras para top categor√≠as
            fig = px.bar(top_categories, 
                        x=top_categories.index, 
                        y="Ingreso_Total",
                        title="Top 5 Categor√≠as por Ingreso Total")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("Bottom 5 Categor√≠as por Ingreso")
            bottom_categories = category_analysis.tail().copy()
            st.dataframe(bottom_categories)
            
            # Gr√°fico de torta para distribuci√≥n
            fig = px.pie(category_analysis, 
                        values="Ingreso_Total", 
                        names=category_analysis.index,
                        title="Distribuci√≥n de Ingresos por Categor√≠a")
            st.plotly_chart(fig, use_container_width=True)
        
        # Insights espec√≠ficos
        st.subheader("üîë Insights Estrat√©gicos")
        top_category = category_analysis.iloc[0]
        bottom_category = category_analysis.iloc[-1]
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Categor√≠a #1", 
                     top_category.name,
                     f"{top_category['%_Contribuci√≥n']}% del total")
        
        with col2:
            st.metric("Ticket Promedio m√°s alto", 
                     f"${category_analysis['Ticket_Promedio'].max():.2f}",
                     f"Categor√≠a: {category_analysis['Ticket_Promedio'].idxmax()}")
        
        with col3:
            st.metric("Categor√≠a menos rentable", 
                     bottom_category.name,
                     f"Solo {bottom_category['%_Contribuci√≥n']}% del total")
    
    # Pregunta 2: Segmentos de clientes
    st.header("2. üë• An√°lisis de Segmentos de Clientes")
    
    customer_segments = analyze_customer_segments(df_clean)
    
    if customer_segments:
        col1, col2 = st.columns(2)
        
        with col1:
            if "ubicacion" in customer_segments:
                st.subheader("üìç Por Ubicaci√≥n")
                loc_data = customer_segments["ubicacion"]
                if isinstance(loc_data, pd.DataFrame):
                    # Aplanar columnas si es multi-index
                    if isinstance(loc_data.columns, pd.MultiIndex):
                        loc_data.columns = ['_'.join(col).strip() for col in loc_data.columns.values]
                    st.dataframe(loc_data)
                    
                    # Gr√°fico de ticket promedio por ubicaci√≥n
                    if any("mean" in col for col in loc_data.columns):
                        mean_col = [col for col in loc_data.columns if "mean" in col][0]
                        fig = px.bar(loc_data, x=loc_data.index, y=mean_col,
                                   title="Ticket Promedio por Ubicaci√≥n")
                        st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if "metodo_pago" in customer_segments:
                st.subheader("üí≥ Por M√©todo de Pago")
                pay_data = customer_segments["metodo_pago"]
                if isinstance(pay_data, pd.DataFrame):
                    # Aplanar columnas si es multi-index
                    if isinstance(pay_data.columns, pd.MultiIndex):
                        pay_data.columns = ['_'.join(col).strip() for col in pay_data.columns.values]
                    st.dataframe(pay_data)
                    
                    # Gr√°fico de distribuci√≥n
                    if any("sum" in col for col in pay_data.columns):
                        sum_col = [col for col in pay_data.columns if "sum" in col][0]
                        fig = px.pie(pay_data, values=sum_col, names=pay_data.index,
                                   title="Distribuci√≥n de Ventas por M√©todo de Pago")
                        st.plotly_chart(fig, use_container_width=True)
        
        # An√°lisis de categor√≠a preferida por cliente
        if "categoria_preferida" in customer_segments:
            st.subheader("üè∑Ô∏è Categor√≠a Preferida por Cliente")
            pref_data = customer_segments["categoria_preferida"]
            if isinstance(pref_data, pd.Series):
                st.dataframe(pref_data.head(10))
                
                fig = px.bar(pref_data.head(10), 
                           x=pref_data.head(10).index,
                           y=pref_data.head(10).values,
                           title="Top 10 Categor√≠as Preferidas por Clientes")
                st.plotly_chart(fig, use_container_width=True)
    
    # Pregunta 3: Patrones temporales
    st.header("3. üìÖ An√°lisis de Patrones Temporales")
    
    temporal_patterns = analyze_temporal_patterns(df_clean)
    
    if temporal_patterns:
        # Gr√°fico de ventas por d√≠a de la semana
        if "dia_semana" in temporal_patterns:
            st.subheader("üìÜ Ventas por D√≠a de la Semana")
            weekday_data = temporal_patterns["dia_semana"]
            st.dataframe(weekday_data)
            
            fig = px.line(weekday_data, x="Day_Name", y="sum",
                         title="Ventas Totales por D√≠a de la Semana")
            st.plotly_chart(fig, use_container_width=True)
        
        # Gr√°fico de ventas por mes
        if "mes" in temporal_patterns:
            st.subheader("üóìÔ∏è Ventas por Mes")
            monthly_data = temporal_patterns["mes"]
            st.dataframe(monthly_data)
            
            fig = px.line(monthly_data, x="Month_Name", y="sum",
                         title="Ventas Totales por Mes")
            st.plotly_chart(fig, use_container_width=True)
        
        # Heatmap de ventas (d√≠a de semana vs hora)
        if "Transaction Date" in df_clean.columns:
            st.subheader("üå°Ô∏è Heatmap de Ventas")
            df_temp = df_clean.copy()
            df_temp["Hour"] = df_temp["Transaction Date"].dt.hour
            df_temp["Weekday"] = df_temp["Transaction Date"].dt.dayofweek
            
            heatmap_data = df_temp.groupby(["Weekday", "Hour"])["Total Spent"].sum().unstack()
            
            fig = px.imshow(heatmap_data,
                          labels=dict(x="Hora del D√≠a", y="D√≠a de la Semana", color="Ventas"),
                          x=[f"{h}:00" for h in range(24)],
                          y=["Lun", "Mar", "Mi√©", "Jue", "Vie", "S√°b", "Dom"],
                          title="Heatmap de Ventas: D√≠a vs Hora")
            st.plotly_chart(fig, use_container_width=True)
    
    # Recomendaciones estrat√©gicas basadas en an√°lisis
    st.header("üéØ Recomendaciones Estrat√©gicas")
    
    rec_col1, rec_col2 = st.columns(2)
    
    with rec_col1:
        st.subheader("üì¶ Para Inventario")
        st.write("""
        1. **Enfocar stock** en categor√≠as de alto ingreso
        2. **Reducir inventario** de categor√≠as de baja rentabilidad
        3. **Negociar mejores t√©rminos** con proveedores de categor√≠as top
        4. **Considerar bundling** de productos de alta y baja rentabilidad
        """)
    
    with rec_col2:
        st.subheader("üë• Para Marketing")
        st.write("""
        1. **Campa√±as personalizadas** para segmentos de alto ticket
        2. **Programas de lealtad** para clientes de ubicaciones espec√≠ficas
        3. **Promociones estrat√©gicas** en d√≠as/horas de baja venta
        4. **Upselling cruzado** basado en categor√≠as preferidas
        """)

# =====================================================
# KPIs PAGE
# =====================================================
elif page == "KPIs":
    st.title("üìä KPIs Dashboard")
    
    # KPIs principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_sales = df_clean['Total Spent'].sum()
        st.metric("üí∞ Total Sales", f"${total_sales:,.0f}")
    
    with col2:
        total_transactions = len(df_clean)
        st.metric("üõí Transactions", f"{total_transactions:,}")
    
    with col3:
        avg_ticket = df_clean['Total Spent'].mean()
        st.metric("üé´ Average Ticket", f"${avg_ticket:,.2f}")
    
    with col4:
        unique_customers = df_clean['Customer ID'].nunique() if 'Customer ID' in df_clean.columns else "N/A"
        st.metric("üë• Unique Customers", f"{unique_customers}")
    
    # KPIs secundarios
    st.subheader("üìà Detailed Metrics")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if "Quantity" in df_clean.columns and "Total Spent" in df_clean.columns:
            total_quantity = df_clean['Quantity'].sum()
            if total_quantity > 0:
                avg_price = df_clean['Total Spent'].sum() / total_quantity
                st.metric("üè∑Ô∏è Average Price per Unit", f"${avg_price:.2f}")
        
        if "Discount Applied" in df_clean.columns:
            discount_rate = df_clean['Discount Applied'].mean() * 100 if df_clean['Discount Applied'].dtype == bool else 0
            st.metric("üéÅ Discount Rate", f"{discount_rate:.1f}%")
    
    with col2:
        if "Payment Method" in df_clean.columns:
            payment_dist = df_clean['Payment Method'].value_counts()
            st.write("üí≥ Payment Method Distribution:")
            st.dataframe(payment_dist)
        
        if "Location" in df_clean.columns:
            location_dist = df_clean['Location'].value_counts()
            st.write("üìç Sales by Location:")
            st.dataframe(location_dist)
    
    # KPIs por categor√≠a
    if "Category" in df_clean.columns:
        st.subheader("üè∑Ô∏è KPIs por Categor√≠a")
        
        category_kpis = df_clean.groupby("Category").agg({
            "Total Spent": ["sum", "mean", "count"],
            "Customer ID": "nunique" if "Customer ID" in df_clean.columns else None
        }).round(2)
        
        st.dataframe(category_kpis)

# =====================================================
# AI INSIGHTS PAGE
# =====================================================
elif page == "AI Insights":
    st.title("ü§ñ AI Generated Insights")
    
    api_key = st.sidebar.text_input("Groq API Key", type="password")
    
    if st.button("üöÄ Generate Insights"):
        if not api_key:
            st.warning("‚ö†Ô∏è Please enter your Groq API Key")
        else:
            # Preparar resumen de datos para el prompt
            data_summary = {
                "shape": df_clean.shape,
                "columns": list(df_clean.columns),
                "numeric_summary": df_clean.describe().to_string(),
                "categorical_summary": df_clean.select_dtypes(include=['object']).describe().to_string() if not df_clean.select_dtypes(include=['object']).empty else "No categorical columns",
                "missing_values": df_clean.isnull().sum().sum()
            }
            
            # An√°lisis de categor√≠as para incluir en el prompt
            category_insights = ""
            if "Category" in df_clean.columns:
                cat_analysis = analyze_category_profitability(df_clean)
                if cat_analysis is not None:
                    category_insights = f"""
AN√ÅLISIS DE CATEGOR√çAS:
{cat_analysis.to_string()}

TOP 3 CATEGOR√çAS:
1. {cat_analysis.index[0]}: ${cat_analysis.iloc[0]['Ingreso_Total']:,.2f} ({cat_analysis.iloc[0]['%_Contribuci√≥n']}%)
2. {cat_analysis.index[1]}: ${cat_analysis.iloc[1]['Ingreso_Total']:,.2f} ({cat_analysis.iloc[1]['%_Contribuci√≥n']}%)
3. {cat_analysis.index[2]}: ${cat_analysis.iloc[2]['Ingreso_Total']:,.2f} ({cat_analysis.iloc[2]['%_Contribuci√≥n']}%)
"""
            
            prompt = f"""
Eres un analista de datos senior especializado en retail. Analiza los siguientes datos y proporciona insights estrat√©gicos:

CONTEXTO DEL DATASET:
- Filas: {data_summary['shape'][0]}
- Columnas: {data_summary['shape'][1]}
- Columnas disponibles: {', '.join(data_summary['columns'])}
- Valores faltantes: {data_summary['missing_values']}

RESUMEN NUM√âRICO:
{data_summary['numeric_summary']}

{category_insights}

TAREAS ESPEC√çFICAS:
1. Identifica los 3 insights m√°s importantes sobre patrones de ventas
2. Detecta 2 riesgos potenciales en los datos o el negocio
3. Proporciona 3 recomendaciones espec√≠ficas para optimizar ventas
4. Formula 1 pregunta estrat√©gica para investigaci√≥n futura
5. Sugiere 2 acciones inmediatas basadas en los datos

Formato tu respuesta en espa√±ol con:
- **üîç Insights Principales**: (lista numerada)
- **‚ö†Ô∏è Riesgos Identificados**: (lista numerada)
- **üéØ Recomendaciones**: (lista numerada)
- **ü§î Pregunta Estrat√©gica**: (una pregunta)
- **üöÄ Acciones Inmediatas**: (lista numerada)
"""

            # Configurar la solicitud a Groq
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "llama3-70b-8192",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            try:
                with st.spinner("üß† Generating AI Insights..."):
                    response = requests.post(
                        "https://api.groq.com/openai/v1/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=120
                    )
                
                if response.status_code == 200:
                    result = response.json()
                    insights = result["choices"][0]["message"]["content"]
                    
                    st.subheader("üí° AI Strategic Insights")
                    st.markdown(insights)
                    
                    # Opci√≥n para descargar insights
                    st.download_button(
                        "üì• Download Insights",
                        insights,
                        file_name=f"ai_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                    )
                    
                else:
                    st.error(f"‚ùå Groq API Error: {response.text}")
                    
            except Exception as e:
                st.error(f"‚ùå Request failed: {str(e)}")
                st.info("üí° Tip: Check your API key and internet connection")